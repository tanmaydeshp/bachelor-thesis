{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2839e1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import filtering  \n",
    "# df = filtering.moses_to_df(\"data/en-si/Ubuntu.en-si.en\", \"data/en-si/Ubuntu.en-si.si\", \"english\", \"sinhala\")\n",
    "# e1 = filtering.to_multilingual_embedding(\"english\", df[\"english\"], \"labse\")\n",
    "# e2 = filtering.to_multilingual_embedding(\"sinhala\", df[\"sinhala\"], \"labse\")\n",
    "# ss = filtering.find_similarity_score(e1, e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3162a60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bucc_style_dataset as bsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e872dc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "si_mono_lines = []\n",
    "eng_mono_lines = []\n",
    "with open(\"data/en-si/sin_wikipedia_2021_30K-sentences.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.removesuffix(\"\\n\")\n",
    "        split  =line.split(\"\\t\")\n",
    "        si_mono_lines.append(split[1])\n",
    "with open(\"data/en-si/eng_news_2024_30K-sentences.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f: \n",
    "        line = line.removesuffix(\"\\n\")\n",
    "        split  =line.split(\"\\t\")\n",
    "        eng_mono_lines.append(split[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96ca0a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "si_par_lines = []\n",
    "eng_par_lines = []\n",
    "with open(\"data/en-si/eng_Latn.dev\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.removesuffix(\"\\n\")\n",
    "        eng_par_lines.append(line)\n",
    "with open(\"data/en-si/sin_Sinh.dev\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f: \n",
    "        line = line.removesuffix(\"\\n\")\n",
    "        si_par_lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f653d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list, test_list = bsd.split_shuffle_create_corpus(eng_mono_lines, si_mono_lines, eng_par_lines, si_par_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "748d6430",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tanmay/miniconda3/envs/thesis_wsl/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "en_list = train_list[0].split(\"\\n\")\n",
    "si_list = train_list[1].split(\"\\n\")\n",
    "en_corpus_lines = []\n",
    "si_corpus_lines = []\n",
    "for line in en_list:\n",
    "    line = line.split(\"\\t\")[1]\n",
    "    en_corpus_lines.append(line)\n",
    "for line in si_list:\n",
    "    line = line.split(\"\\t\")[1]\n",
    "    si_corpus_lines.append(line)\n",
    "import filtering\n",
    "e1 = filtering.to_multilingual_embedding(\"english\", en_corpus_lines, \"labse\")\n",
    "e2 = filtering.to_multilingual_embedding(\"sinhala\", si_corpus_lines, \"labse\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fd150ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_lines = [f\"{e1.shape[0]} {e1.shape[1]}\"]\n",
    "target_lines = [f\"{e2.shape[0]} {e2.shape[1]}\"]\n",
    "for sent, encoding in zip(en_corpus_lines, e1):\n",
    "    sent = sent.replace(\" \", \"_\")\n",
    "    encoding_str = \" \".join([f\"{x:.4f}\" for x in encoding])\n",
    "    source_lines.append(f\"{sent} {encoding_str}\")\n",
    "for sent, encoding in zip(si_corpus_lines, e2):\n",
    "    sent = sent.replace(\" \", \"_\")\n",
    "    encoding_str = \" \".join([f\"{x:.4f}\" for x in encoding])\n",
    "    target_lines.append(f\"{sent} {encoding_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71d83c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"source.vec\", \"w\", encoding=\"utf-8\") as f: \n",
    "    for line in source_lines:\n",
    "        f.write(f\"{line}\\n\")\n",
    "with open(\"target.vec\", \"w\", encoding=\"utf-8\") as f: \n",
    "    for line in target_lines:\n",
    "        f.write(f\"{line}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b94ad43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Running search query...\n",
      "WARNING:root:Saving results...\n"
     ]
    }
   ],
   "source": [
    "import bilingual_nearest_neighbor as bnn \n",
    "bnn.main(source_embeddings=\"evaluation/en.source.vec\", target_embeddings=\"evaluation/si.target.vec\", output=\"evaluation/en-si.output.txt\", binary=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87e54ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_file_dict = {}\n",
    "with open(\"data/en-si/eng_Latn.dev\", \"r\", encoding=\"utf-8\") as f1, open(\"data/en-si/sin_Sinh.dev\", \"r\", encoding=\"utf-8\") as f2:\n",
    "    for line1, line2 in zip(f1, f2):\n",
    "        line1 = line1.removesuffix(\"\\n\")\n",
    "        line2 = line2.removesuffix(\"\\n\")\n",
    "        line1 = line1.replace(\"\\u200d\", \"\")\n",
    "        line2 = line2.replace(\"\\u200d\", \"\")\n",
    "        gold_file_dict[line1] = line2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f1d5774",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"evaluation/en-si.output.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    test_dict = {}\n",
    "    for line in f:\n",
    "        split = line.split(\"\\t\")\n",
    "        split = [item.replace(\"_\", \" \") for item in split]\n",
    "        split = [item.replace(\"\\u200d\", \"\") for item in split]\n",
    "        split = split[:2]\n",
    "        if split[0] in gold_file_dict.keys():\n",
    "            test_dict[split[0]] = split[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4741d8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"evaluation/mined.en\", \"w\", encoding=\"utf-8\") as f1, open(\"evaluation/mined.si\", \"w\", encoding=\"utf-8\") as f2:\n",
    "    for s1, s2 in test_dict.items():\n",
    "        f1.write(s1 + \"\\n\")\n",
    "        f2.write(s2 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d375d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 156/156 [00:00<00:00, 570454.60it/s]\n",
      "100%|██████████| 156/156 [00:00<00:00, 572450.94it/s]\n",
      "2025-05-17 10:47:38,159 - simalign.simalign - INFO - Initialized the EmbeddingLoader with model: xlm-roberta-base\n",
      "INFO:simalign.simalign:Initialized the EmbeddingLoader with model: xlm-roberta-base\n",
      "100%|██████████| 156/156 [00:21<00:00,  7.33it/s]\n",
      "2025-05-17 10:48:01,832 - simalign.simalign - INFO - Initialized the EmbeddingLoader with model: xlm-roberta-base\n",
      "INFO:simalign.simalign:Initialized the EmbeddingLoader with model: xlm-roberta-base\n",
      "100%|██████████| 156/156 [00:00<00:00, 3013.34it/s]\n",
      "100%|██████████| 156/156 [00:00<00:00, 638976.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw corpus: 249\n",
      "\n",
      "After dropping duplicates: 249\n",
      "\n",
      "After removing length based outliers: 247\n",
      "\n",
      "After performing language identification: 247\n",
      "\n",
      "After filtering based on similarity scores: 156\n",
      "\n",
      "After filtering based on word alignment: 149\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import filtering \n",
    "filtering.main(files=[\"evaluation/mined.en\", \"evaluation/mined.si\"], langs=[\"english\", \"sinhala\"], output=\"evaluation/filtered.en-si.tsv\", model=\"labse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "544618ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "output_df = pd.read_csv(\"evaluation/filtered.en-si.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa4a0ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149\n",
      "\n",
      "0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "wrong = 0\n",
    "for sentence in output_df[\"english\"]:\n",
    "    if sentence in gold_file_dict: \n",
    "        if output_df.loc[output_df[\"english\"] == sentence, \"sinhala\"].iloc[0] == gold_file_dict[sentence]:\n",
    "            correct = correct + 1\n",
    "        else: \n",
    "            wrong = wrong + 1 \n",
    "print(f\"{correct}\\n\")\n",
    "print(f\"{wrong}\\n\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_wsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
