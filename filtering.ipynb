{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e872dc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create synthetic parallel corpora to evaluate pipeline by injecting real parallel sentences into different monolingual corpora'''\n",
    "\n",
    "import bucc_style_dataset as bsd\n",
    "\n",
    "si_mono_lines = []\n",
    "eng_mono_lines = []\n",
    "with open(\"data/en-si/sin_wikipedia_2021_30K-sentences.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.removesuffix(\"\\n\")\n",
    "        line = line.replace(\"\\u200d\", \"\")\n",
    "        split  =line.split(\"\\t\")\n",
    "        si_mono_lines.append(split[1])\n",
    "with open(\"data/en-si/eng_news_2024_30K-sentences.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f: \n",
    "        line = line.removesuffix(\"\\n\")\n",
    "        line = line.replace(\"\\u200d\", \"\")\n",
    "        split  =line.split(\"\\t\")\n",
    "        eng_mono_lines.append(split[1])\n",
    "\n",
    "si_par_lines = []\n",
    "eng_par_lines = []\n",
    "\n",
    "with open(\"data/en-si/eng_Latn.dev\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.removesuffix(\"\\n\")\n",
    "        line = line.replace(\"\\u200d\", \"\")\n",
    "        eng_par_lines.append(line)\n",
    "with open(\"data/en-si/sin_Sinh.dev\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f: \n",
    "        line = line.removesuffix(\"\\n\")\n",
    "        line = line.replace(\"\\u200d\", \"\")\n",
    "        si_par_lines.append(line)\n",
    "\n",
    "train_list, test_list = bsd.split_shuffle_create_corpus(eng_mono_lines, si_mono_lines, eng_par_lines, si_par_lines)\n",
    "\n",
    "en_list = train_list[0].split(\"\\n\")\n",
    "si_list = train_list[1].split(\"\\n\")\n",
    "en_corpus_lines = []\n",
    "si_corpus_lines = []\n",
    "for line in en_list:\n",
    "    line = line.split(\"\\t\")[1]\n",
    "    en_corpus_lines.append(line)\n",
    "for line in si_list:\n",
    "    line = line.split(\"\\t\")[1]\n",
    "    si_corpus_lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "748d6430",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tanmay/miniconda3/envs/thesis_wsl/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-20 15:37:01,664 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
      "2025-05-20 15:37:01,905 | INFO | laser_encoders.download_models |  - laser2.spm already downloaded\n",
      "2025-05-20 15:37:01,961 | INFO | laser_encoders.download_models |  - laser2.pt already downloaded\n",
      "2025-05-20 15:37:01,962 | INFO | laser_encoders.download_models |  - laser2.spm already downloaded\n",
      "2025-05-20 15:37:01,963 | INFO | laser_encoders.download_models |  - laser2.cvocab already downloaded\n",
      "2025-05-20 15:37:02,507 | INFO | laser_encoders.download_models |  - laser2.spm already downloaded\n",
      "2025-05-20 15:37:02,570 | INFO | laser_encoders.download_models |  - laser3-sin_Sinh.v1.pt already downloaded\n",
      "2025-05-20 15:37:02,571 | INFO | laser_encoders.download_models |  - laser2.spm already downloaded\n",
      "2025-05-20 15:37:02,572 | INFO | laser_encoders.download_models |  - laser2.cvocab already downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 243/243 [04:18<00:00,  1.07s/it]\n",
      "Batches: 100%|██████████| 243/243 [05:13<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-20 15:46:40,242 | INFO | faiss.loader | Loading faiss with AVX2 support.\n",
      "2025-05-20 15:46:40,284 | INFO | faiss.loader | Successfully loaded faiss with AVX2 support.\n",
      "2025-05-20 15:46:40,288 | INFO | gensim.models.keyedvectors | loading projection weights from evaluation/en.source.vec\n",
      "2025-05-20 15:46:41,712 | INFO | gensim.utils | KeyedVectors lifecycle event {'msg': 'loaded (7749, 768) matrix of type float32 from evaluation/en.source.vec', 'binary': 0, 'encoding': 'utf-8', 'datetime': '2025-05-20T15:46:41.712880', 'gensim': '4.3.3', 'python': '3.9.21 (main, Dec 11 2024, 16:24:11) \\n[GCC 11.2.0]', 'platform': 'Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.39', 'event': 'load_word2vec_format'}\n",
      "2025-05-20 15:46:41,713 | INFO | gensim.models.keyedvectors | loading projection weights from evaluation/si.target.vec\n",
      "2025-05-20 15:46:43,116 | INFO | gensim.utils | KeyedVectors lifecycle event {'msg': 'loaded (7749, 768) matrix of type float32 from evaluation/si.target.vec', 'binary': 0, 'encoding': 'utf-8', 'datetime': '2025-05-20T15:46:43.116208', 'gensim': '4.3.3', 'python': '3.9.21 (main, Dec 11 2024, 16:24:11) \\n[GCC 11.2.0]', 'platform': 'Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.39', 'event': 'load_word2vec_format'}\n",
      "2025-05-20 15:46:43,156 | WARNING | root | Running search query...\n",
      "2025-05-20 15:46:43,645 | WARNING | root | Saving results...\n"
     ]
    }
   ],
   "source": [
    "'''Convert synthetic corpora to Word2Vec format to perform sentence mining'''\n",
    "\n",
    "import filtering\n",
    "e1 = filtering.to_multilingual_embedding(\"english\", en_corpus_lines, \"labse\")\n",
    "e2 = filtering.to_multilingual_embedding(\"sinhala\", si_corpus_lines, \"labse\")  \n",
    "source_lines = [f\"{e1.shape[0]} {e1.shape[1]}\"]\n",
    "target_lines = [f\"{e2.shape[0]} {e2.shape[1]}\"]\n",
    "\n",
    "for sent, encoding in zip(en_corpus_lines, e1):\n",
    "    sent = sent.replace(\" \", \"_\")\n",
    "    encoding_str = \" \".join([f\"{x:.4f}\" for x in encoding])\n",
    "    source_lines.append(f\"{sent} {encoding_str}\")\n",
    "for sent, encoding in zip(si_corpus_lines, e2):\n",
    "    sent = sent.replace(\" \", \"_\")\n",
    "    encoding_str = \" \".join([f\"{x:.4f}\" for x in encoding])\n",
    "    target_lines.append(f\"{sent} {encoding_str}\")  \n",
    "\n",
    "with open(\"evaluation/en.source.vec\", \"w\", encoding=\"utf-8\") as f: \n",
    "    for line in source_lines:\n",
    "        f.write(f\"{line}\\n\")\n",
    "with open(\"evaluation/si.target.vec\", \"w\", encoding=\"utf-8\") as f: \n",
    "    for line in target_lines:\n",
    "        f.write(f\"{line}\\n\")\n",
    "        \n",
    "import bilingual_nearest_neighbor as bnn \n",
    "bnn.main(source_embeddings=\"evaluation/en.source.vec\", target_embeddings=\"evaluation/si.target.vec\", output=\"evaluation/en-si.mined.txt\", binary=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f136b446",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 16/16 [00:14<00:00,  1.10it/s]\n",
      "Batches: 100%|██████████| 16/16 [00:23<00:00,  1.47s/it]\n",
      "Batches: 100%|██████████| 16/16 [00:15<00:00,  1.04it/s]\n",
      "Batches: 100%|██████████| 16/16 [00:27<00:00,  1.69s/it]\n",
      "Batches: 100%|██████████| 16/16 [00:15<00:00,  1.03it/s]\n",
      "Batches: 100%|██████████| 16/16 [00:24<00:00,  1.52s/it]\n",
      "Batches: 100%|██████████| 16/16 [00:14<00:00,  1.09it/s]\n",
      "Batches: 100%|██████████| 16/16 [00:24<00:00,  1.54s/it]\n",
      "Batches: 100%|██████████| 16/16 [00:17<00:00,  1.12s/it]\n",
      "Batches: 100%|██████████| 16/16 [00:23<00:00,  1.49s/it]\n",
      "Batches: 100%|██████████| 16/16 [00:17<00:00,  1.12s/it]\n",
      "Batches: 100%|██████████| 16/16 [00:26<00:00,  1.65s/it]\n",
      "Batches: 100%|██████████| 16/16 [00:16<00:00,  1.05s/it]\n",
      "Batches: 100%|██████████| 16/16 [00:27<00:00,  1.70s/it]\n",
      "Batches: 100%|██████████| 16/16 [00:17<00:00,  1.07s/it]\n",
      "Batches: 100%|██████████| 16/16 [00:26<00:00,  1.65s/it]\n",
      "Batches: 100%|██████████| 16/16 [00:16<00:00,  1.05s/it]\n",
      "Batches: 100%|██████████| 16/16 [00:21<00:00,  1.36s/it]\n",
      "Batches: 100%|██████████| 16/16 [00:17<00:00,  1.11s/it]\n",
      "Batches: 100%|██████████| 16/16 [00:22<00:00,  1.43s/it]\n",
      "Batches: 100%|██████████| 16/16 [00:16<00:00,  1.06s/it]\n",
      "Batches: 100%|██████████| 16/16 [00:24<00:00,  1.56s/it]\n",
      "Batches: 100%|██████████| 12/12 [00:11<00:00,  1.09it/s]\n",
      "Batches: 100%|██████████| 12/12 [00:16<00:00,  1.39s/it]\n",
      "100%|██████████| 2784/2784 [00:00<00:00, 1230058.18it/s]\n",
      "100%|██████████| 2784/2784 [00:00<00:00, 820298.02it/s]\n",
      "2025-05-20 16:03:12,185 - simalign.simalign - INFO - Initialized the EmbeddingLoader with model: xlm-roberta-base\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-20 16:03:12,185 | INFO | simalign.simalign | Initialized the EmbeddingLoader with model: xlm-roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2784/2784 [05:46<00:00,  8.03it/s]\n",
      "2025-05-20 16:09:02,881 - simalign.simalign - INFO - Initialized the EmbeddingLoader with model: xlm-roberta-base\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-20 16:09:02,881 | INFO | simalign.simalign | Initialized the EmbeddingLoader with model: xlm-roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2784/2784 [00:00<00:00, 4552.13it/s]\n",
      "100%|██████████| 2784/2784 [00:00<00:00, 2015002.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw corpus: 7749\n",
      "\n",
      "After dropping duplicates: 7749\n",
      "\n",
      "After removing length based outliers: 6029\n",
      "\n",
      "After performing language identification: 6008\n",
      "\n",
      "After filtering based on similarity scores: 2784\n",
      "\n",
      "After filtering based on word alignment: 862\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''Apply pipeline to the mined text'''\n",
    "\n",
    "en_mined = []\n",
    "si_mined = []\n",
    "with open(\"evaluation/en-si.mined.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f: \n",
    "        split = line.split(\"\\t\")\n",
    "        en_line = split[0]\n",
    "        si_line = split[1]\n",
    "        en_line = en_line.replace(\"\\u200d\", \"\")\n",
    "        en_line = en_line.removesuffix(\"\\n\")\n",
    "        si_line = si_line.replace(\"\\u200d\", \"\")\n",
    "        si_line = si_line.removesuffix(\"\\n\")\n",
    "        en_line = en_line.replace(\"_\", \" \")\n",
    "        si_line = si_line.replace(\"_\", \" \")\n",
    "        en_mined.append(en_line)\n",
    "        si_mined.append(si_line)\n",
    "\n",
    "with open(\"evaluation/en.mined.txt\", \"w\", encoding=\"utf-8\") as f1, open(\"evaluation/si.mined.txt\", \"w\", encoding=\"utf-8\") as f2:\n",
    "    for l1 in en_mined:\n",
    "        f1.write(f\"{l1}\\n\")\n",
    "    for l2 in si_mined:\n",
    "        f2.write(f\"{l2}\\n\")\n",
    "\n",
    "filtering.main(files=[\"evaluation/en.mined.txt\", \"evaluation/si.mined.txt\"], langs=[\"english\", \"sinhala\"], model=\"labse\", output=\"evaluation/en-si.filtered.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee33f77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Generate gold file and test file from the filtered data and evaluate using the bucc_f-score script'''\n",
    "\n",
    "gold_dict = {}\n",
    "with open(\"evaluation/en-si.gold.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for source, target in zip(eng_par_lines, si_par_lines):\n",
    "        gold_dict[source] = target\n",
    "        f.write(f\"{source}\\t{target}\\n\")\n",
    "\n",
    "en_lines = []\n",
    "si_lines = []\n",
    "with open(\"evaluation/en-si.filtered.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.replace(\"\\u200d\", \"\")\n",
    "        line = line.removesuffix(\"\\n\")\n",
    "        split = line.split(\"\\t\")\n",
    "        en_lines.append(split[1])\n",
    "        si_lines.append(split[2])\n",
    "\n",
    "with open(\"evaluation/en-si.test.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for line1, line2 in zip(en_lines[1:], si_lines[1:]):\n",
    "        f.write(f\"{line1}\\t{line2}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ab2c21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_wsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
